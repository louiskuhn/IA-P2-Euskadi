{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Analyse et modélisation de séries temporelles**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Notions de bases**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.1. Définition**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une série temporelle (ou chronologique), est une suite de valeurs numériques représentant l’évolution d’une quantité au cours du temps. On la note généralement $\\{y_t\\}_{t=1}^{T}$.  \n",
    "NB: on parle parfois de processus ou processus stochastique qui fait référence au processus, **inconnu et non observable**, qui a entraîné ces observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.2. Les composantes d'une série temporelle**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une série temporelle peut se décomposer en 4 composantes :\n",
    "- **Tendance** : orientation générale de la série (vers le haut ou vers le bas)\n",
    "- **Saisonnalité** : tendances hebdomadaires, mensuelles, trismestrielles ou annuelles\n",
    "- **Cycle** : cycles économiques à long terme, sur plusieurs années\n",
    "- **Bruit** : ce qui reste après avoir extrait les composants précédents\n",
    "\n",
    "On verra plus loin comment décomposer une série temporelle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.3. La stationnarité**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![stationnarite](img/stationnarite.png)\n",
    "\n",
    "Un processus $\\{y_t\\}_{t=1}^{T}$ est dit stationnaire lorsque : \n",
    "- $E(y_t) = \\mu$\n",
    "- $V(y_t) = \\sigma^2$\n",
    "- $\\gamma(t,s) = \\gamma(t-s)$ où $\\gamma$ est la fonction d'autocorrélation qu'on va découvrir de suite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.4. L'autocorrélation et l'autocorrélation partielle**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'autocorrélation est la covariance entre deux valeurs d'une même série :\n",
    "$$\\gamma(t,s) = Cov(y_t,y_s) = E[(y_t-\\mu_t)(y_s-\\mu_s)]$$\n",
    "Cet indicateur montre comment les observations d'une série temporelle sont liées entre elles.\n",
    "\n",
    "La **fonction d'autocorrélation (ACF)** et la **fonction d'autocorrélation partielle (PACF)** permettent de mesurer l'association entre des valeurs actuelles et passées. Elles indiquent les valeurs passées les plus corrélées aux valeurs suivantes et donc les plus utiles à la prévision de valeurs futures. Pour parler _\"avec les mains\"_, la fonction ACF mesure la similitude entre 2 observations en fonction du décalage temporel entre les 2.\n",
    "\n",
    "Concrètement :\n",
    "- ACF au décalage k = corrélation entre les valeurs séparées par k intervalles\n",
    "- PACF au décalage k = corrélation entre les valeurs séparées par k intervalles, compte tenu des valeurs des intervalles intermédiaires\n",
    "\n",
    "![acfpacf](img/acf_pacf.png)\n",
    "\n",
    "Ces graphiques interviennent dans la détermination de l'ordre des processus d'un modèle ARIMA qu'on va voir bientôt car je suis sûr que vous avez hââââââte."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.5. Bruit blanc**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un **bruit blanc** est un processus $\\{\\epsilon_t\\}_{t=1}^{T}$ qui vérifie :\n",
    ">- $E[\\epsilon_t] = 0$\n",
    ">- $E[\\epsilon_t^2] = \\sigma^2$\n",
    ">- $E[\\epsilon_t\\epsilon_s] = 0$ pour $s \\neq t$\n",
    "\n",
    "Un **bruit blanc indépendant** est un processus $\\{\\epsilon_t\\}_{t=1}^{T}$ qui vérifie :\n",
    ">- $E[\\epsilon_t] = 0$\n",
    ">- $E[\\epsilon_t^2] = \\sigma^2$\n",
    ">- $\\epsilon_t$ et $\\epsilon_s$ indépendants pour $s \\neq t$\n",
    "\n",
    "Un **bruit blanc gaussien** $\\{\\epsilon_t\\}_{t=1}^{T}$ est un bruit blanc indépendant qui suit une loi normale centrée : $\\epsilon_t \\hookrightarrow \\mathcal{N}(0,\\sigma^2)$\n",
    "\n",
    "Intuitivement, un bruit blanc dans le cadre des séries temporelles, c'est la partie purement aléatoire du processus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    " \n",
    "# on génère un bruit blanc gaussien\n",
    "e = np.random.normal(size=1000)\n",
    "\n",
    "# on visualise la série avec la fonction définie dans le script fonctions.py\n",
    "import fonctions as fct\n",
    "fct.ts_plot(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interprétation :** les graphiques ci-dessus ressemblent à la structure souhaitée d'une série chronologique d'erreurs :\n",
    "- pas de pics dans les graphes ACF et PACF \n",
    "- le QQ-plot et l'histogramme indiquent un comportement normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.6. La marche aléatoire**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un processus de marche aléatoire $\\{x_t\\}_{t=1}^{T}$ est une série qui vérifie :  \n",
    "$x_t = x_{t-1} + \\epsilon_t$ avec $\\epsilon_t$ un bruit blanc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paramètres\n",
    "T = 1000\n",
    "e = np.random.normal(size=T)\n",
    "x = np.zeros_like(e)\n",
    " \n",
    "#simulation d'une marche aléatoire\n",
    "for t in range(T):\n",
    "    x[t] = x[t-1] + e[t]\n",
    "    \n",
    "#visualisation\n",
    "fct.ts_plot(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interprétation :** on voit la ressemblance entre ce processus de marche aléatoire et les cours d'actifs en bourse qui sont caractérisés par : \n",
    "- un ACF élevé pour toute longueur de décalage et un PACF élevé pour le premier décalage uniquement \n",
    "- un QQ-plot et un histogramme indiquant que la série n'est pas un bruit blanc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.7. Modèle autorégressif**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une série temporelle $\\{x_t\\}_{t=1}^{T}$ qui suit un modèle autorégressif d'ordre $p$ (et noté $AR(p)$) est telle que :  \n",
    "$x_t = \\mu + \\Sigma_{i=1}^{p}\\phi_i x_{t-i} + \\epsilon_t$ avec $\\mu$ une constante et $\\epsilon_t$ un bruit blanc\n",
    "\n",
    "Un processus autorégressif d'ordre 1 est donc donné par :\n",
    "$AR(1)$ : $x_t = \\mu + \\phi x_{t-1} + \\epsilon_t$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paramètres\n",
    "T = 1000\n",
    "e = np.random.normal(size=T)\n",
    "x = np.zeros_like(e)\n",
    "phi = 0.3\n",
    " \n",
    "#simulation d'un processus AR(1)\n",
    "for t in range(T):\n",
    "    x[t] = phi * x[t-1] + e[t]\n",
    "    \n",
    "#visualisation\n",
    "fct.ts_plot(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interprétation :**\n",
    "- pour un processus AR(1) avec $\\phi=0.3$,on voit sur les graphiques ACF et PACF, un pic au premier décalage qui tend de suite vers 0\n",
    "- l'histogramme et le QQ-plot indiquent des résidus presque aléatoires.\n",
    "\n",
    "*Remarque pas anodine* : la marche aléatoire introduite précédemment est un processus AR(1), et il n'est pas stationnaire. Par contre, la série différenciée $(x_t - x_{t-1})$ est stationnaire. La stationnarité d'un processus $AR(1)$ dépend de si $|\\phi|<1$. On peut généraliser cette propriété aux processus $AR(p)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.8. Modèle moyenne mobile**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le modèle de moyenne mobile d'ordre $q$ noté $MA(q)$ suppose que la série temporelle observée peut être représentée par une combinaison linéaire de termes d'erreur de bruit blanc. Cela s'écrit donc :  \n",
    "$x_t = \\epsilon_t + \\Sigma_{i=1}^{q}\\theta_i\\epsilon_{t-i}$ avec $\\epsilon_t$ un bruit blanc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paramètres\n",
    "T = 1000\n",
    "e = np.random.normal(size=T)\n",
    "x = np.zeros_like(e)\n",
    "theta1 = 0.8\n",
    "theta2 = -1.4\n",
    " \n",
    "# simulation d'un modèle MA(2)\n",
    "for t in range(T):\n",
    "    x[t] = e[t] + theta1 * e[t-1] + theta2 * e[t-2]\n",
    "\n",
    "#visualisation\n",
    "fct.ts_plot(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interprétation :** pour le processus $MA(2)$ avec les paramètres choisis, on voit un pic plus important pour le deuxième décalage dans les tracés ACF et PACF. \n",
    "On peut observer également que le pic dans le graphique PACF ne décroît pas immédiatement mais prend quelques périodes pour se \"désintégrer\".\n",
    "\n",
    "*Remarque toujours pas anodine* : un processus $MA(q)$ est toujours stationnaire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.9. Modèle autorégressif et moyenne mobile**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un processus $ARMA(p,q)$ est une combinaison des deux modèles précédents et s'écrit donc :  \n",
    "$x_t = \\epsilon_t + \\Sigma_{i=1}^{p}\\phi_i x_{t-i} + \\Sigma_{i=1}^{q}\\theta_i\\epsilon_{t-i}$ avec $\\epsilon_t$ un bruit blanc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paramètres\n",
    "T = 1000\n",
    "e = np.random.normal(size=T)\n",
    "x = np.zeros_like(e)\n",
    "phi1 = 0.3\n",
    "phi2 = 0.6\n",
    "theta1 = 0.8\n",
    "theta2 = -1.1\n",
    "theta3 = 0.5\n",
    "\n",
    "# simulation d'un modèle ARMA(2,3)\n",
    "for t in range(T):\n",
    "    x[t] = e[t] + phi1 * x[t-1] + phi2 * x[t-2] + theta1 * e[t-1] + theta2 * e[t-2] + theta3 * e[t-3]\n",
    "\n",
    "#visualisation\n",
    "fct.ts_plot(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interprétation :** on voit clairement que la série n'est pas stationnaire et donc on n'ira pas beaucoup plus loin pour l'interprétation..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Des exemples de séries temporelles**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importer, afficher et commenter les séries temporelles :\n",
    "- \"sunspot\" : nombre annuel de tâches solaires de 1790 à 1970\n",
    "- \"uspop\" : population des Etats-Unis, en millions, de 1790 à 1990 (fréquence décennal)\n",
    "- \"beer\" : production mensuelle de bière en Australie, en mégalitres, de janvier 1956 à aout 1995\n",
    "- \"lynx\" : nombre annuel de lynx capturés au Canada, de 1821 à 1934"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sunspot = pd.read_csv(\"data/sunspot.csv\", index_col=\"t\")\n",
    "sunspot.plot(y=\"sunspot\", figsize=(14,4));\n",
    "#fct.ts_plot(sunspot[\"sunspot\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uspop = pd.read_csv(\"data/uspop.csv\", index_col=\"t\")\n",
    "uspop.plot(y=\"uspop\", figsize=(14,4));\n",
    "#fct.ts_plot(uspop[\"uspop\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beer = pd.read_csv(\"data/beer.csv\", header=None, names=[\"t\", \"Beer\"], parse_dates=True, dayfirst=True, index_col=\"t\")\n",
    "beer.plot(y=\"Beer\", figsize=(14,4));\n",
    "#fct.ts_plot(beer[\"Beer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lynx = pd.read_csv(\"data/lynx.csv\", index_col=\"t\")\n",
    "lynx.plot(y=\"Lynx\", figsize=(14,4));\n",
    "#fct.ts_plot(beer[\"Beer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Décomposition d'une série temporelle**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le principe de la décomposition des séries temporelles est de découper les séries originales en plusieurs composants indépendants (qu'on a vus plus haut) : tendance, saisonnalité, cycle et bruit.\n",
    "\n",
    "Pour illustrer tout ça, on va utiliser des données qui sont aux séries temporelles ce que le dataset \"Titanic\" est au machine learning, la base : la série AirPassenger qui mesure le nombre mensuel de passagers aériens, en milliers, de janvier 1949 à décembre 1960.\n",
    "\n",
    "Dans la suite, on va noter $X_{t}$  la série AirPassengers et $Y_t=ln(X_t)$. \n",
    "\n",
    "**Exo :** pour commencer, charger le dataset et afficher simplement la série (sans utiliser la fonction `ts_plot`) puis afficher aussi la série logarithmique. Commenter les résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.1. La tendance**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On l'a vu un dans le notebook d'intro, une façon d'identifier la tendance consiste à lisser la courbe en utilisant des moyennes mobiles sur une fenêtre glissante d'observations.\n",
    "\n",
    "**Exo :** faire une figure avec plusieurs graphiques, chacun représentant 2 séries : la série originale et la série des moyennes mobiles pour différentes longueur de fenêtres glissantes. Commenter les résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.2. Saisonnalité**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une manière de vérifier l'effet saisonnier est de tracer une courbe pour chaque année en prenant les mois comme abscisses et visualiser ainsi la répartition des passagers au cours de chaque année. Just do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.3. Bruit**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exo :** utiliser la fonction `seasonal_decompose` qui renvoie la série temporelle d'origine, la tendance, la saisonnalité et les résidus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Prévision avec un lissage exponentiel**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le lisage exponentiel est une technique de lissage qui ressemble un peu aux moyennes mobiles et peut être utilisée pour prédire une série temporelle. C'est en fait un cas particulier des modèles ARMA déjà évoqués. On en distingue différents types (simple, double, triple...). On notera que les lissages exponentiels simple et double ne sont plus vraiment d'actualité. En revanche le lissage exponentiel triple, lui, est parfois utilisé.\n",
    "\n",
    "Pour un lissage exponentiel simple (qui ne sera pas efficace ici puiqu'il s'applique sur une série sans saisonnalité et à tendance localement constante), on peut faire :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.api import ExponentialSmoothing\n",
    "\n",
    "les = ExponentialSmoothing(y.values, trend=None, seasonal=None).fit()\n",
    "les_pred = les.forecast(12)\n",
    "\n",
    "plt.figure(figsize=(14,6))\n",
    "plt.plot(y, label='Airpass')\n",
    "plt.plot(pd.date_range(y.index[len(y)-1], periods=12, freq='M'), les_pred, label='Pred')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour le lissage exponentiel double (adapté pour des séries à tendance localement linéaire) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "led = ExponentialSmoothing(y.values, trend='add', seasonal=None).fit()\n",
    "led_pred = led.forecast(12)\n",
    "\n",
    "plt.figure(figsize=(14,6))\n",
    "plt.plot(y, label='Airpass')\n",
    "plt.plot(pd.date_range(y.index[len(y)-1], periods=12, freq='M'), led_pred, label='Pred')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour le lissage exponentiel triple ou méthode de Holt-Winters (prise en compte en plus d'une composante saisonnière) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let = ExponentialSmoothing(y.values, seasonal_periods=12, trend='add', seasonal='add').fit()\n",
    "y_pred = let.forecast(60)\n",
    "x_pred = np.exp(let.forecast(60))\n",
    "\n",
    "\n",
    "fig, (ax1,ax2) = plt.subplots(2,1, figsize=(14,12))\n",
    "\n",
    "ax1.plot(y, label='Airpass')\n",
    "ax1.plot(pd.date_range(y.index[len(y)-1], periods=60, freq='M'), y_pred, label='Pred')\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(x, label='Airpass')\n",
    "ax2.plot(pd.date_range(y.index[len(y)-1], periods=60, freq='M'), x_pred, label='Pred')\n",
    "ax2.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. Prévision à l'aide d'un modèle ARIMA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un modèle autoréfressif intégré à moyennes mobiles (ARIMA) est une généralisation du modèle ARMA, aperçu plus haut. \n",
    "\n",
    "Les modèles ARIMA sont appliqués dans certains cas où les données montrent une non-stationnarité, où une étape de différenciation initiale (correspondant à la partie \"intégrée\" du modèle) peut être appliquée une ou plusieurs fois pour éliminer la non-stationnarité.\n",
    "\n",
    "Dans le modèle de base, trois paramètres $(p,d,q)$ sont utilisés pour paramétrer les modèles ARIMA. Par conséquent, un modèle ARIMA est noté $ARIMA(p,d,q)$ et est défini par :\n",
    "$$\n",
    "\\left(1 - \\sum_{i=1}^p \\phi_i L^i \\right) (1 - L)^d y_t = \\mu + \\left(1 + \\sum_{i=1}^q \\theta_i L^i \\right) \\varepsilon_t\n",
    "$$\n",
    "\n",
    "Une généralisation est le modèle SARIMA, ou ARIMA saisonnier, qui permet de tenir compte en plus de la saisonnalité. Ce modèle est caractérisé non plus par 3 paramètres $(p,d,q)$ mais par 7 ! Le modèle SARIMA noté $SARIMA(p,d,q)(P,D,Q)_s$ où :\n",
    "- $s$ est la période de la saisonnalité\n",
    "- $(P,D,Q)$ sont les ordres de la partie saisonnière\n",
    "\n",
    "On va essayer de pas trop rentrer dans les détails théoriques mais de voir par la pratique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5.1. Stationnarisation de la série**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les séries ont généralement une tendance et une saisonnalité. Pour pouvoir les modéliser par des processus stationnaires comme les ARMA, il faut les stationnariser. On va voir en pratique comment faire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fct.plot_acf_pacf(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La sortie ACF présente une décroissance lente vers 0, ce qui traduit un problème de non-stationnarité. On effectue donc une différenciation $(1-L)$ c'est-à-dire qu'on va regarder la série $Y_t-Y_{t-1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_diff_1 = y - y.shift(1)\n",
    "fct.plot_acf_pacf(y_diff_1[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Toujours une décroissance lente vers 0 de la série différenciée d'ordre 1 **pour les multiples de 12**. Cela correspond à la saisonnalité et on va donc différencier à nouveau en appliquant cette fois $(1-L^{12})$ ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_diff_1_12 = y_diff_1 - y_diff_1.shift(12)\n",
    "fct.plot_acf_pacf(y_diff_1_12[13:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec cette double différenciation, on s'approche d'un autocorrélogramme simple empirique. On va donc modéliser la série $(1-L)(1-L^{12})ln(X_t)$ par un modèle ARMA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5.2. Identification, estimation et validation de modèles**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va s'appuyer sur les sorties ACF et PACF donc on peut utiliser une des fonctions disponibles dans `fonctions.py` par exemple `ts_plot` pour changer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_diff_1_12[13:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fct.ts_plot(y_diff_1_12[13:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les graphiques ACF et PACF permettent de voir empiriquement qu'on va avoir un modèle $SARIMA(p,d,q)(P,D,Q)_s$ avec $s=12$. On va tester différents modèles.\n",
    "\n",
    "**Modèle 1 :** pour commencer on va prendre un $SARIMA(1,1,1)(1,1,1)_{12}$ qui s'écrit :\n",
    "$$(1-\\phi_{1}L)(1-\\phi'_{1}L^{12})(1-L)(1-L^{12})ln(X_{t})=(1+\\theta_{1}L)(1+\\theta'_{1}L^{12})\\epsilon_{t} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.statespace.sarimax import *\n",
    "\n",
    "m1 = SARIMAX(y.values, order=(1,1,1), seasonal_order=(1,1,1,12))\n",
    "res1 = m1.fit()\n",
    "print(res1.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce modèle a des coefficients non significatifs, on en teste un second en retirant dans le paramètre associé à la plus grande p-value : ici c'est le terme autorégressif saisonnier.\n",
    "\n",
    "**Modèle 2 :** on va donc prendre un $SARIMA(1,1,1)(0,1,1)_{12}$ qui s'écrit :\n",
    "$$(1-\\phi_{1}L)(1-L)(1-L^{12})ln(X_{t})=(1+\\theta_{1}L)(1+\\theta'_{1}L^{12})\\epsilon_{t} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2 = SARIMAX(y.values, order=(1,1,1), seasonal_order=(0,1,1,12))\n",
    "res2 = m2.fit()\n",
    "print(res2.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce modèle a toujours un coefficient non significatif, on en teste un 3ème modèle en retirant dans le terme autorégressif d'ordre 1.\n",
    "\n",
    "**Modèle 3 :** on va donc prendre un $SARIMA(0,1,1)(0,1,1)_{12}$ qui s'écrit :\n",
    "$$(1-L)(1-L^{12})ln(X_{t})=(1+\\theta_{1}L)(1+\\theta'_{1}L^{12})\\epsilon_{t} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m3 = SARIMAX(y.values, order=(0,1,1), seasonal_order=(0,1,1,12))\n",
    "res3 = m3.fit()\n",
    "print(res3.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette fois tous les coefficients sont significatifs, il faut encore tester ce qu'on appelle la \"blancheur\" des résidus : on veut avoir des résidus qui sont un bruit blanc (faible), c'est-à-dire, entre autres qu'ils sont non corrélés.  \n",
    "Pour ça on peut utiliser le test de Ljung-Box d'autocorrélation des résidus qui est implémenté dans la fonction `acorr_ljungbox` de `statsmodels.stats.diagnostic` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "print('Retard : p-value')\n",
    "for lag in [6, 12, 18, 24, 30, 36]:\n",
    "    print('{} : {}'.format(lag, acorr_ljungbox(res3.resid, lags=lag, return_df=False)[1].mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Toutes les p-values sont élevées ce qui signifie qu'on ne peut pas rejeter l'hypothèse nulle du test. Or, l'hypothèse $H_0$ est qu'il n'y a pas d'autocorrélation des résidus, on peut pas la rejeter, donc on est bon !\n",
    "\n",
    "On peut enfin tester la normalité des résidus avec le test de Shapiro-Wilk implémenté dans la fonction `scipy.stats.shapiro`, l'hypothèse nulle étant que les étant que les observations sont normalement distribuées, on souhaite encore une fois une p-value élevée pour ne pas rejeter $H_0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import shapiro\n",
    "shapiro(res3.resid[13:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le test de normalité est également validé pour ce modèle et on peut afficher les résidus pour le visualiser :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(res3.resid[13:], bins=10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autre méthode d'affichage des résultats\n",
    "# attention, ce sont les résidus standardisés par contre donc pas exactement les mêmes que ceux affichés au dessus.\n",
    "res3.plot_diagnostics(figsize=(16, 10));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5.3. Prévision de l’année 1961 avec le modèle retenu**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prédiction et intervalles de confiance\n",
    "y_pred = res3.get_forecast(12)\n",
    "x_pred = np.exp(y_pred.predicted_mean)\n",
    "x_pred_inf = [np.exp(p[0]) for p in y_pred.conf_int(alpha=0.05)]\n",
    "x_pred_sup = [np.exp(p[1]) for p in y_pred.conf_int(alpha=0.05)]\n",
    "\n",
    "# affichage\n",
    "plt.figure(figsize=(14,6))\n",
    "plt.plot(x.index, x.values, label='Passagers')\n",
    "plt.plot(pd.date_range(x.index[-1], periods=12, freq='M'), x_pred, color='r', label='Predit')\n",
    "plt.plot(pd.date_range(x.index[-1], periods=12, freq='M'), x_pred_inf, color='g', linestyle='--')\n",
    "plt.plot(pd.date_range(x.index[-1], periods=12, freq='M'), x_pred_sup, color='g', linestyle='--')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5.4. Évaluation de la qualité prédictive du modèle**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a pas eu vraiment besoin de le faire jusqu'à maintenant mais, vous le savez, pour pouvoir faire un modèle prédictif, il faut mettre en place, entraîner, tester et évaluer votre modèle. Pour cela, il faut découper nos données en training et test sets.\n",
    "\n",
    "**Exo :** splitter en training et test sets en tronquant la série en 1959 pour pouvoir ensuite prédire l'année 1960"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exo :** vérifier que le modèle retenu précédemment est toujours valide sur la série tronquée \"y_train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exo :** prédire les valeurs des années 1959 et 1960 et afficher sur un même graphique les valeurs réelles, prédites et les intervalles de confiance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exo :** calculer les 2 métriques RMSE et MAPE pour ces prédictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exo :** répeter les opérations du point 5.4 en modifiant la troncature : training set avec les données jusqu'en 1958 pour prédire un test set avec les années 1959 et 1960"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5.5. Une autre approche pour la détermination du modèle**\n",
    "\n",
    "Pour adapter les données de série temporelles à un modèle ARIMA saisonnier avec les paramètres $SARIMA(p,d,q)(P,D,Q)s$, on doit trouver les paramètres optimaux et pour ça on peut aussi faire de la recherche sur grille qui consiste à tester de manière itérative plusieurs valeurs possibles des paramètres et d'évaluer les modèles en utilisant des critères comme le critère AIC ou BIC.\n",
    "\n",
    "On utilisera nous le critère d'information Akaike ($AIC$) qui mesure la qualité relative des modèles statistiques pour un ensemble d'observations donné. À partir d'un ensemble de modèles, le critère $AIC$ permet d'estime la qualité de chacun des modèle par rapport aux autres modèles. Cela permet donc de sélectionner des modèles. Intuitivement, le critère $AIC$ mesure le compromis entre la qualité de l'ajustement du modèle aux observations fournies et la complexité du modèle (nombre de paramètres inclus et estimés). Le critère $AIC$ est calculé par :\n",
    "\n",
    "$AIC=2k-2ln(L)$ avec :\n",
    "- $k$ nombre de paramètres estimés\n",
    "- $L$ valeur maximale de la vraisemblance (pour simplifier maximiser la vraisemblance revient plus ou moins à minimiser l'erreur)\n",
    "\n",
    "Le meilleur modèle est celui qui minimise le critère $AIC$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exo :** on continue à modéliser la série logarithmique. Tronquer la série en 1958 (inclus) pour prédire 1959 et 1960"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exo :** générer les valeurs possibles des paramètres que nous aimerions évaluer, à savoir $(p,d,q)(P,D,Q)$ puisque $s$ sera égal à 12 on le sait. On testera toutes les valeurs entre 0 et 2 pour chacun des 6 paramètres c'est-à-dire $3^6 = 729$ possibilités, c'est long..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exo :** boucler sur toutes les combinaisons possibles de paramètres et calculer le critère AIC pour déterminer le meilleur modèle parmi ceux testés (la valeur du critère AIC d'un modèle est facilement récupérable...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le code ci-dessus permet de tester toutes les valeurs des paramètres et on obtient comme modèle optimal (c'est un peu long).  \n",
    "output : Modèle retenu : SARIMA(0, 1, 1)(1, 0, 1)12 - AIC:-405.4220183264598"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exo :**\n",
    "- entrainer le modèle retenu\n",
    "- calculer l'AIC\n",
    "- afficher le summary pour notamment vérifier les p-values des coefficients\n",
    "- regarder les résidus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exo :** prédire à partir du modèle retenu les 2 années 1959 et 1960 et afficher sur un même graphique la série originale, les prédictions et l'intervalle de confiance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exo :** évaluer la qualité prédictive du modèle via RMSE et MAPE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exo :** faire une prévision à long terme  \n",
    "À l'aide d'un processus ARIMA saisonnier basé sur toute la série temporelle (on modélise toujours la série logarithmique), prédire les 10 années suivantes : déterminer les paramètres du modèle optimal par recherche sur grille.  \n",
    "Afficher les graphiques qui vous paraissent utiles et intéressants. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
