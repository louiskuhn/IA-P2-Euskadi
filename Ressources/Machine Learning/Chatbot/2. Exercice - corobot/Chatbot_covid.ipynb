{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fccb3fc9-9663-4e11-b511-1a5a21e4c252",
   "metadata": {},
   "source": [
    "# Chatbot infos corona\n",
    "\n",
    "Dans ce cas pratique on va essayer d'entraîner un chatbot qui permette de répondre automatiquement aux questions de l'utilisateur sur le coronavirus à partir de multiples informations que l'on a regroupées dans le fichier `infos_corona.txt`. L'idée est que le chatbot renvoie la ou les phrases utilisant le plus de termes similaires à ceux utilisés dans la question de l'utilisateur.   \n",
    "La démarche est similaire à celle présentée dans le notebook `chatbot_wiki`.  \n",
    "\n",
    "- En utilisant la fonction `open` (et en précisant le paramètre `encoding` adéquat!), importez le texte dans une chaîne de caractères que vous nommerez `texte`.  \n",
    "- Procédez à quelques nettoyages : \n",
    "    - passez le texte en minuscules.  \n",
    "    - vérifiez s'il n'y pas d'acronyme pouvant fausser notre tokenization en phrases, si oui, remplacez-le par autre chose.  \n",
    "    - le virus est appelé covid-19 ou coronavirus, faites en sorte qu'un terme unique soit utilisé.   \n",
    "- Avec la fonction `nltk.sent_tokenize`, passez votre chaîne de caractères `texte` en une liste de phrases que vous appelerez `phrases_token`.  \n",
    "- Il y a beaucoup de questions dans cette liste, or on veut des réponses. Supprimez-les.\n",
    "- Récupérez un vecteur de stop words français avec la fonction `get_stop_words` du module `stop_words` \n",
    "\n",
    "On a maintenant tout ce dont on a besoin pour faire notre matrice TF-IDF! Vous pouvez déjà la fitter sur vos infos :  \n",
    "- Stockez le résultat de `TfidfVectorizer` en fixant le paramètre `stop_words` avec les stop words français.  \n",
    "- Fittez la fonction sur votre liste de phrases `phrases_token` et stockez ce résultat dans un objet `tf_idf_chat`.  \n",
    "\n",
    "Il faut maintenant définir une fonction que vous appelerez dans votre chatbot. Celle-ci doit :  \n",
    "- Prendre en entrée la phrase entrée par l'utilisateur et la mettre dans une liste.    \n",
    "- Créer la matrice TF-IDF des infos avec `tf_idf_chat.transform()`. \n",
    "- Créer la matrice TF-IDF pour la phrase de l'utilisateur avec `tf_idf_chat.transform()`.  \n",
    "- Calculez la similarité entre la phrase de l'utilisateur et le reste des phrases avec `sklearn.metrics.pairwise.cosine_similarity`.  \n",
    "- Renvoyer en réponse la phrase avec la similarité la plus grande, ou un message d'erreur s'il n'y a pas de similarité. _Alternative un peu plus complexe_ : Vous pouvez aussi renvoyer plusieurs phrases si plusieurs sont similaires, en les concaténant dans une même chaîne de caractère avec '\\n'.join().  \n",
    "\n",
    "Maintenant il n'y a plus qu'à intégrer cette fonction dans votre chatbot et le tester. N'oubliez pas de lui faire dire bonjour et de laisser la possibilité à l'utilisateur de quitter!\n",
    "\n",
    "## Rendez votre chatbot encore plus intelligent  \n",
    "\n",
    "Donnez la possibilité à l'utilisateur de nourrir le chatbot de nouvelles informations. Vous pouvez par exemple déterminer qu'après avoir tapé \"infos\", l'utilisateur va rentrer une phrase que vous devrez ajouter aux possibilités de réponses de votre chatbot.  \n",
    "\n",
    "## Bonus avec stemmatizer\n",
    "\n",
    "On définit maintenant une fonction à indiquer dans le paramètre `tokenizer` de la fonction `TfidfVectorizer`. Cette fonction doit récupérer la racine des mots plutôt que les mots en entier pour trouver des correspondances entre des mots de la même racine même s'ils ne sont pas écrits sous la même forme.  \n",
    "- Utilisez la fonction `FrenchStemmer` de `nltk.stem.snowball` pour définir :  \n",
    "    - une première fonction qui renvoie la liste des mots stemmatisés quand on rentre une liste de mots.  \n",
    "    - Une fonction qui applique cette première fonction à une phrase qu'on tokenize en mots (avec `nltk.word_tokenize`)  \n",
    "Vous pouvez tester cette fonction sur des phrases pour voir si elle fait bien ce que vous désirez.  \n",
    "- Ajoutez l'appel à cette fonction dans `TfidfVectorizer`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0c3d875-c742-43db-9e95-a1d6e195e19b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elka/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['alor', 'aur', 'aurion', 'auron', 'auss', 'autr', 'avi', 'avion', 'avon', 'ayon', 'cec', 'cel', 'chaqu', 'comm', 'dan', 'dedan', 'dehor', 'devr', 'devrion', 'devron', 'droit', 'e', 'encor', 'euss', 'eussion', 'eûm', 'fair', 'forc', 'fuss', 'fussion', 'fûm', 'hor', 'just', 'mainten', 'moin', 'mêm', 'nomm', 'notr', 'parc', 'parol', 'person', 'san', 'ser', 'serion', 'seron', 'seul', 'somm', 'soyon', 'tand', 'tel', 'tres', 'votr', 'éti', 'étion', 'ête'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Corona-bot : Je suis le corona-bot, je réponds à vos questions sur l'épidémie ! \n",
      "Si vous voulez ajouter des infos supplémentaires (de sources vérifiées bien sûr !) vous pouvez taper 'infos'.\n",
      "Pour quitter, vous pouvez juste me dire au revoir.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "> Vous :   quels sont les gestes barrières\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Corona-bot : Je diffuse régulièrement les gestes barrières et les recommandations du ministère des Solidarités et de la Santé.\n",
      "Comme pour l’épisode de grippe saisonnière, ce sont les gestes barrières qui sont efficaces.\n",
      "C’est donc pourquoi les gestes barrières et les mesures de distanciation sociale sont indispensables pour se protéger de la maladie.\n",
      "Ce sont les gestes barrières et la distanciation sociale qui sont efficaces.\n",
      "C’est pour cela qu’il est important de respecter les gestes barrières et les mesures de distanciation sociale.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "> Vous :   combien va durer l'épidémie de covid ?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Corona-bot : Je ne suis pas sûr d'avoir trouvé exactement ce que vous vouliez dire, voilà ce que j'ai trouvé : \n",
      "Il va être mis en place très rapidement, en lien avec Air France.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "> Vous :   infos\n",
      "> Vos informations :   l'épidémie de virus va durer 15 ans\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merci c'est noté!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "> Vous :   combien va durer l'épidémie de covid .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Corona-bot : l'épidémie de virus va durer 15 ans\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "> Vous :   salut\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Corona-bot : Bonjour, bienvenue sur ce chatbot!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "> Vous :   au revoir\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Corona-bot : au revoir!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "with open('infos_corona.txt','r',errors = 'ignore', encoding = \"utf8\") as f:\n",
    "    texte = f.read()\n",
    "\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('wordnet')\n",
    "\n",
    "# on supprime l'acronyme n.c.a, on corrige : \n",
    "texte = re.sub('n.c.a.', 'nca', texte)\n",
    "\n",
    "# Tokenisation en phrases et mots\n",
    "phrases_token = nltk.sent_tokenize(texte, language = \"french\")\n",
    "\n",
    "# on a beaucoup de questions ici : on les enlève\n",
    "for i in sorted(range(len(phrases_token)), reverse = True):\n",
    "    if re.search(r\"\\?\", phrases_token[i]):\n",
    "        del phrases_token[i]\n",
    "\n",
    "# on enlève les doublons\n",
    "phrases_token = list(set(phrases_token)) \n",
    "\n",
    "# On crée une liste nettoyée mais qui ne sera pas celle dans laquelle\n",
    "# on ira chercher les réponses, simplement pour la création de la\n",
    "# matrice TF-IDF\n",
    "def nettoyage(texte):\n",
    "    texte = texte.lower()\n",
    "    # on remplace covid-19 par coronavirus\n",
    "    texte = re.sub('covid-19| virus|covid |sars-cov', 'coronavirus', texte)\n",
    "    # on remplace les \"coronavirus coronavirus\" par coronavirus\n",
    "    texte = re.sub('coronavirus coronavirus', 'coronavirus', texte)\n",
    "    texte = re.sub(f\"[{string.punctuation}]\", \" \", texte)\n",
    "    texte = re.sub('[éèê]', 'e', texte)\n",
    "    texte = re.sub('[àâ]', 'a', texte)\n",
    "    texte = re.sub('[ô]', 'o', texte)\n",
    "    texte = re.sub('mort(\\w){0,3}|deces|deced(\\w){1,5}', 'deces', texte)\n",
    "    texte = re.sub('remedes?|traitements?|antidotes?', 'traitement', texte)\n",
    "    texte = re.sub('medec(\\w){1,5}|medic(\\w){1,3}', 'medical', texte)\n",
    "    return texte\n",
    "\n",
    "phrases_nettoyees = []\n",
    "for i in range(len(phrases_token)):\n",
    "    phrases_nettoyees.append(nettoyage(phrases_token[i]))\n",
    "      \n",
    "# on récupère les stop words\n",
    "from stop_words import get_stop_words\n",
    "french_stop_words = get_stop_words('french')\n",
    "\n",
    "# Stemmer : on prend la racine des mots, \n",
    "# Lemmer : on ramène à une forme plus simple mais existante : infinitif pour les verbes, le singulier pour un nom...\n",
    "from nltk.stem.snowball import FrenchStemmer\n",
    "french_stem = FrenchStemmer()\n",
    "\n",
    "def stem_tokens(tokens):\n",
    "    return [french_stem.stem(token) for token in tokens]\n",
    "    \n",
    "def stem_norm(text):\n",
    "    return stem_tokens(nltk.word_tokenize(text))\n",
    "    #return re.sub(f\"[{string.punctuation}]\", \" \", stem_tok)\n",
    "\n",
    "\n",
    "# générer des réponses à partir de la matrice tf-idf\n",
    "tf_idf = TfidfVectorizer(tokenizer=stem_norm, token_pattern=None, stop_words = french_stop_words)\n",
    "tf_idf_chat = tf_idf.fit(phrases_nettoyees)\n",
    "\n",
    "def reponse_corona(user_sentence):\n",
    "    user_sentence = [user_sentence]\n",
    "    phrases_tf = tf_idf_chat.transform(phrases_nettoyees)\n",
    "    user_tf = tf_idf_chat.transform(user_sentence)\n",
    "    similarity = cosine_similarity(user_tf, phrases_tf).flatten()\n",
    "    index_max_sim = np.argmax(similarity)\n",
    "    if(similarity[index_max_sim] == 0):\n",
    "        robo_response = \"Je n'ai pas trouvé cette information, désolé!\"\n",
    "    elif(similarity[index_max_sim] <= 0.30):\n",
    "        robo_response = \"\"\"Je ne suis pas sûr d'avoir trouvé exactement ce que vous vouliez dire, voilà ce que j'ai trouvé : \\n\"\"\"+phrases_token[index_max_sim] \n",
    "    else:\n",
    "        simil_index = []\n",
    "        for i in range(len(similarity)):\n",
    "            if similarity[i] > 0.3:\n",
    "                simil_index.append(i)\n",
    "        robo_response = '\\n'.join([phrases_token[i] for i in simil_index])\n",
    "    return robo_response\n",
    "\n",
    "# gérer les salutations\n",
    "salutations_user = r\"bonjour.*|salut.*|hello.*|hey.*|coucou.*|bonsoir.*\"\n",
    "salutations_robot = [\"Bonjour, bienvenue sur ce chatbot!\"]\n",
    "def salutations(user_sentence):\n",
    "    if re.fullmatch(salutations_user, user_sentence):\n",
    "        return random.choice(salutations_robot)\n",
    "    \n",
    "# gérer les demandes de nouvelles\n",
    "nouvelles_user = r\".*[çs]a va.*\\?|.*la pêche\\?|.*la forme\\?\"\n",
    "nouvelles_robot = [\"Je suis un robot, ça va jamais vraiment\",\n",
    "                   \"Un peu marre du confinement\",\n",
    "                   \"On fait aller!\",\n",
    "                   \"J'ai une pêche d'enfer!!!\"]\n",
    "def nouvelles(user_sentence):\n",
    "    if re.fullmatch(nouvelles_user, user_sentence):\n",
    "        return random.choice(nouvelles_robot)\n",
    "    \n",
    "# On définit une porte de sortie pour l'utilisateur\n",
    "exit_user = [\"au revoir\", \"bye\", \"bye bye\", \"à +\", \"ciao\"]\n",
    "exit_bot = [\"au revoir!\", \"j'espère vous avoir été utile!\",\"à une prochaine fois :)\"]\n",
    "\n",
    "# on définit enfin notee chatbot : \n",
    "flag = True\n",
    "print(\"\"\"> Corona-bot : Je suis le corona-bot, je réponds à vos questions sur l'épidémie ! \n",
    "Si vous voulez ajouter des infos supplémentaires (de sources vérifiées bien sûr !) vous pouvez taper 'infos'.\n",
    "Pour quitter, vous pouvez juste me dire au revoir.\"\"\")\n",
    "while flag:\n",
    "    user_sentence = input(\"> Vous :  \")\n",
    "    user_sentence = user_sentence.lower()\n",
    "    if (user_sentence == \"infos\"):\n",
    "        user_info = input(\"> Vos informations :  \")\n",
    "        phrases_token.append(user_info)\n",
    "        user_info = user_info.lower()\n",
    "        user_info = nettoyage(user_info)\n",
    "        phrases_nettoyees.append(user_info) \n",
    "        tf_idf_chat = tf_idf.fit(phrases_nettoyees)\n",
    "        print(\"Merci c'est noté!\")\n",
    "    elif not (user_sentence in exit_user):\n",
    "        if (salutations(user_sentence) != None):\n",
    "            print(\"> Corona-bot : \" + salutations(user_sentence))\n",
    "        elif (nouvelles(user_sentence) != None):\n",
    "            print(\"> Corona-bot : \" + nouvelles(user_sentence))\n",
    "        else:\n",
    "            user_sentence = nettoyage(user_sentence)\n",
    "            print(\"> Corona-bot : \" + reponse_corona(user_sentence))\n",
    "    else:\n",
    "        flag = False\n",
    "        print(\"> Corona-bot : \" + random.choice(exit_bot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb4f70e-64eb-49dc-a421-a27b29f9a486",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('base': conda)",
   "language": "python",
   "name": "python3812jvsc74a57bd0addda3f63a324b7a38ec870d79a82ab21d33100c2675b6c03c06d4beeb089079"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
