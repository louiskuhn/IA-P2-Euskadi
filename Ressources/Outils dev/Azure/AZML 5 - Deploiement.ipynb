{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mise en production avec Azure ml service\n",
    "Laurent Cetinsoy\n",
    "\n",
    "Avoir des modèles, c'est bien, qu'ils soient utilisable en production, c'est mieux. \n",
    "Dans ce notebook on va s'amuser (ou galérer, c'est selon votre rapport à la vie) à mettre en production un modèle entraîné. \n",
    "\n",
    "La référence ci dessous va pouvoir nous servir: https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-and-where\n",
    "\n",
    "\n",
    "Les classes et notions intéressantes sont : WebService, InferenceConfig, le script de scoring (que j'appellerai plutôt de prédiction)\n",
    "\n",
    "\n",
    "Autre référence à regarder pour l'executer par batch : \n",
    "  - https://docs.microsoft.com/fr-fr/azure/machine-learning/how-to-use-parallel-run-step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#charger ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mise en production et  Web services\n",
    "\n",
    "Il existe plusieurs manière de mettre en production via un web service. Cela ce traduit par le fait que la classe WebService a plusieurs classes filles. Quelles sont-elles ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LocalWebService, AciWebService"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script de scoring\n",
    "\n",
    "Le script de \"scoring\" permet de spécifier comment le azure ml service va utiliser votre modèle pour faire des prédiction chaque fois que le webservice reçoit une requête. \n",
    "\n",
    "ref utile : https://docs.microsoft.com/en-us/learn/modules/register-and-deploy-model-with-amls/2-deploy-model (section 2)\n",
    "\n",
    "Dans ce script il y a deux fonctions à compléter : les deux fonctions init (pour charger votre beau modèle) et run. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Créer un environnement avec la méthode from_conda_specification et le fichier env.yaml qui avait été fourni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Créer une instance de la classe **InferenceConfig** qui utilise le script fake_predict.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avant de déployer le script en local ou sur azure. Assurez vous que vos fonction init et run marchent bien \n",
    "en lançant le fichier de prediction avec python directement. Afficher le résultat de la fonction run. Cela doit renvoyer la valeur 42. Est ce que ça marche"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce magnifique faux modèle renvoie toujours 42. On va le déployer avec un azure machine learning. D'abord en local et ensuite sur un vrai service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Déploiement local avec LocalWebService\n",
    "\n",
    "\n",
    "**Si ça marche pas en local** créer un notebok sur Azure ou passer directement à la mise en production \"normale\" et ne perdez pas trop de temps. \n",
    "\n",
    "Il est conseillé de lancer les commandes suivantes dans un fichier python pour avoir les feedback\n",
    "\n",
    "Pour déployer un WebService, on créer d'abord un objet configuration. \n",
    "\n",
    "Importer la classe LocalWebService "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec la méthode deploy_configuration, créer un objet configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec la méthode **deploy** de la classe Model, \"deployez le modèle\" localement.  Attention ça demande docker, donc pour les personnes sur windows... essayez Windows subsystem linux mais sinon ne perdez pas trop de temps et sauvegarder le resultat dans la variable **service**. \n",
    "Vous pouvez mettre une liste vide pour l'arguments \"models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utiliser la méthode wait_for_deployment sur votre service et voir si ça marche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afficher l'url du webservice pour envoyer des requêtes pour faire une prédiction à l'aide de l'attribut **scoring_uri** de votre web_service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faire une requête http avec la librairie request (ou reprendre celle ci) et vérifier que ça renvoie bien **42**: \n",
    "\n",
    "ref utile https://docs.microsoft.com/fr-fr/azure/machine-learning/how-to-consume-web-service#call-the-service-python\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant que ça marche on va déployer notre magnifique modèle qui retourne toujours 42 sur azure ! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afficher les logs de votre service avec la méthode **get_logs** de votre service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le module logging de python est un module qui permet de collecter et centraliser les logs. \n",
    "\n",
    "il est utilisé par azml. \n",
    "\n",
    "\n",
    "si vous avez un problème executer la cellule suivante :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quoi sert-elle ? cf https://docs.microsoft.com/fr-fr/azure/machine-learning/how-to-enable-logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faux modèle déployé sur azure container instance\n",
    "\n",
    "Il existe plusieurs manières de déployer un modèle dans un webservice sur azure. \n",
    "Dans un premier temps on va utiliser les Azure Container instance. \n",
    "\n",
    "En gros, notre notre modèle sera dans un webservice (une sorte de flask), \n",
    "lequel sera dans executé dans un conteneur (docker probablement).\n",
    "\n",
    "\n",
    "Reference utile : https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-azure-container-instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importer la classe AciWebService qui représente un web service tournant dans une Azure Container instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Créer un objet deployment_config avec la méthode AciWebservice.deploy_configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Toujours avec le même objet inference config, deployer le modèle dans un AciWebservice avec la méthode **deploy** de la classe **Model** et récupérer le résultat dans la variable *service*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Appeler la méthode wait_for_deployment avec l'argument show_output=True et attendre longtemps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vérifier sur le portail azure que le déploiement est bien encours en attendant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une fois que c'est déployé, récupérer l'url où il faut envoyer la requête avec l'attribut scoring_uri d'objet service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Envoyer une requête avec votre service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activer l'authentification sur votre service pour que votre modèle ne soit pas accèssible pour tout le monde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ajouter le header authentication à votre requête avec la clé d'authentication pour que les requête marchent avec l'authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus : déploiement d'un vrai modèle sur azure instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va, en gros, reprendre les même étapes que précédemment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Etape 0 (optionel si vous avez déjà un modèle entraîné sur azure). Entraîner un modèle simple et le register sur Azure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Etape 1 : faire une copie du script predict_fake et modifier la copie : \n",
    "   - pour qu'il charge un modèle depuis votre workspace dans la fonction init\n",
    "   - que lla fonction run utilisé le modèle qui a été chargé dans la variable gloabale model pour faire les prediction avec les données \"data\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Etap2 : refaire la procédure précédente pour que le modèle soit déployé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
