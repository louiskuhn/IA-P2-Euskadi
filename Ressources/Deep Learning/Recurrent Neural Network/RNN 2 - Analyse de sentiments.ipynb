{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Recurrent Neural Network - Analyse de sentiments**\n",
    "\n",
    "Le but de cet exercice est la classification binaire (positive ou négative) de reviews en utilisant un RNN sur un dataset de commentaires IMDB que vous trouverez dans le dossier data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import des librairies\n",
    "import string\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Import des données et preprocessing**\n",
    "\n",
    "1. Importer les 2 fichiers \".txt\" de data/imdb\n",
    "2. Vérifier ce qu'il y a dedans si c'est pas déjà fait...\n",
    "3. Un peu de preprocessing de texte, allez on va chercher dans sa petite mémoire et sinon dans son gros ordinateur :\n",
    ">- convertir en minuscules\n",
    ">- retirer la ponctuation\n",
    ">- créer une liste des reviews et déterminer combien il y en a. Pareil pour les labels. Inch'Allah y en aura autant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import des txt\n",
    "with open('data/imdb/reviews.txt', 'r') as f:\n",
    "    reviews_str = f.read()\n",
    "with open('data/imdb/labels.txt', 'r') as f:\n",
    "    labels_str = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(str,\n",
       " 33678267,\n",
       " 'bromwell high is a cartoon comedy . it ran at the same time as some other programs about school life  such as  teachers  . my   years in the teaching ')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reviews\n",
    "type(reviews_str), len(reviews_str), reviews_str[:150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(str,\n",
       " 225000,\n",
       " 'positive\\nnegative\\npositive\\nnegative\\npositive\\nnegative\\npositive\\nnegative\\npositive\\nnegative\\npositive\\nnegative\\npositive\\nnegative\\npositive\\nnegative\\npositi')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# labels\n",
    "type(labels_str), len(labels_str), labels_str[:150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bromwell high is a cartoon comedy  it ran at the same time as some other programs about school life  such as  teachers   my   years in the teaching profession lead me to believe that bromwell high  s satire is much closer to reality than is  teachers   the scramble to survive financially  the insightful students who can see right through their pathetic teachers  pomp  the pettiness of the whole situation  all remind me of the schools i knew and their students  when i saw the episode in which a student repeatedly tried to burn down the school  i immediately recalled          at           high  a classic line inspector i  m here to sack one of your teachers  student welcome to bromwell high  i expect that many adults of my age think that bromwell high is far fetched  what a pity that it isn  t   \\nstory of a man who has unnatural feelings for a pig  starts out with a opening scene that is a terrific example of absurd comedy  a formal orchestra audience is turned into an insane  violent mob by the crazy chantings of it  s singers  unfortunately it stays absurd the whole time with no general narrative eventually making it just too off putting  even those from the era should be turned off  the cryptic dialogue would make shakespeare seem easy to a third grader  on a technical level it  s better than you might think with some good cinematography by future great vilmos zsigmond  future stars sally kirkland and frederic forrest can be seen briefly   \\nhomelessness  or houselessness as george carlin stated  has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school  work  or vote for the matter  most people think of the homeless as just a lost cause while worrying about things such as racism  the war on iraq  pressuring kids to succeed  technology  the elections  inflation  or worrying if they  ll be next to end up on the streets   br    br   but what if you were given a bet to live on the st'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# netoyage des string\n",
    "reviews_str = reviews_str.lower()\n",
    "reviews_str = reviews_str.translate(str.maketrans('', '', string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25001, 25001)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split et nb de lables\n",
    "rvws = reviews_str.split('\\n')\n",
    "labs = labels_str.split('\\n')\n",
    "len(rvws), len(labs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Tokenisation ou encodage**\n",
    "\n",
    "Le but est de remplacer les mots des reviews par des entiers. Si vous voulez faire à votre sauce, vous y êtes encouragés !\n",
    "\n",
    "Sinon, les quelques étapes décrites ci-dessous vous permettront de le faire :\n",
    ">- compter l'ensemble des mots (vous pouvez utiliser `Counter` de la librairie `collections` qui est une des plus rapide dans ce domaine)\n",
    ">- les trier par ordre décroissant d'occurrences\n",
    ">- créer un dictionnaire `word_mapping` où les clés sont les mots et les valeurs l'entier associé (votre dico doit être `{'the': 1, 'and': 2, 'a': 3, 'of': 4,..., 'muppified': 74070, 'whelk': 74071, 'hued': 74072}}`). C'est volontaire que ça commence à 1 car on utilisera le \"0\" comme caractère spécial (il servira pour \"remplir\" les reviews les plus courtes afin qu'elles aient toutes la même taille...)\n",
    ">- encoder les mots (c'est-à-dire les remplacer par l'entier qui les représente)\n",
    ">- encoder les labels (ça c'est fastoche, on se débrouille)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comptage des mots et tri par occurrence\n",
    "words = reviews_str.split()\n",
    "countwords = Counter(words)\n",
    "sorted_words = sorted(countwords, key=countwords.get, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionnaire de correspondance mot=entier\n",
    "word_mapping = {w:i+1 for i,w in enumerate(sorted_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[21025, 308, 6, 3, 1050, 207, 8, 2138, 32, 1, 171, 57, 15, 49, 81, 5785, 44, 382, 110, 140, 15, 5194, 60, 154, 9, 1, 4975, 5852, 475, 71, 5, 260, 12, 21025, 308, 13, 1978, 6, 74, 2395, 5, 613, 73, 6, 5194, 1, 24103, 5, 1983, 10166, 1, 5786, 1499, 36, 51, 66, 204, 145, 67, 1199, 5194, 19869, 1, 37442, 4, 1, 221, 883, 31, 2988, 71, 4, 1, 5787, 10, 686, 2, 67, 1499, 54, 10, 216, 1, 383, 9, 62, 3, 1406, 3686, 783, 5, 3483, 180, 1, 382, 10, 1212, 13583, 32, 308, 3, 349, 341, 2913, 10, 143, 127, 5, 7690, 30, 4, 129, 5194, 1406, 2326, 5, 21025, 308, 10, 528, 12, 109, 1448, 4, 60, 543, 102, 12, 21025, 308, 6, 227, 4146, 48, 3, 2211, 12, 8, 215, 23], [63, 4, 3, 125, 36, 47, 7472, 1395, 16, 3, 4181, 505, 45, 17, 3, 622, 134, 12, 6, 3, 1279, 457, 4, 1721, 207, 3, 10624, 7373, 300, 6, 667, 83, 35, 2116, 1086, 2989, 34, 1, 898, 46417, 4, 8, 13, 5096, 464, 8, 2656, 1721, 1, 221, 57, 17, 58, 794, 1297, 832, 228, 8, 43, 98, 123, 1469, 59, 147, 38, 1, 963, 142, 29, 667, 123, 1, 13584, 410, 61, 94, 1774, 306, 755, 5, 3, 819, 10396, 22, 3, 1724, 635, 8, 13, 128, 73, 21, 233, 102, 17, 49, 50, 617, 34, 682, 85, 28785, 28786, 682, 374, 3341, 11398, 2, 16371, 7946, 51, 29, 108, 3324]]\n"
     ]
    }
   ],
   "source": [
    "# encodage des mots dans les reviews\n",
    "rvws_num = [list(map(lambda w : word_mapping[w], rvw.split())) for rvw in rvws]\n",
    "print(rvws_num[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encodage des labels\n",
    "labs_num = [1*(lab == 'positive') for lab in labs]\n",
    "labs_num[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Longueur des séquences**\n",
    "\n",
    "Il s'agit ici d'analyser la longueur des reviews pour déterminer éventuellement des outliers et choisir ce qu'on en fait. On va aussi \"uniformiser\" la longueur des séquence.\n",
    "\n",
    "1. Avec la méthode de votre choix (graphique, stats desc...), étudier la longueur des reviews\n",
    "2. Déterminer s'il y a des outliers et ce que vous souhaitez en faire (si vous les décidez de les supprimer, attention de bien supprimer aussi les labels correspondants...)\n",
    "3. Ça a été évoqué un peu plus haut, on veut que nos reviews aient toutes la même taille pour faciliter l'entraînement du réseau. Par conséquent on va ajouter des 0 aux reviews les plus courtes et tronquer les reviews les plus longues (*padding/truncating*, si vous vous souvenez bien on a vu il y a peu le *zero-padding* dans un certain cas...bon ben c'est pareil).  \n",
    ">- définir une fonction `trunc_pad(review_list, length)` :\n",
    ">>- qui prend en paramètres la liste des reviews (chaque review étant encodée en une liste d'entiers) et une longueur donnée\n",
    ">>- et qui retourne un array 2D avec (en ligne) les reviews trop longues tronquées et des 0 à gauche pour les reviews trop courtes. Avant de vous lancer et pour vous assurer d'avoir compris, quelles dimensions doit avoir votre array en sortie ?\n",
    ">>- tester votre fonction avec une longueur fixée à 250 et afficher les 5 premières valeurs des 5 premières reviews. Vous devez obtenir ça:  \n",
    "\\[[    0     0     0     0     0]  \n",
    "[    0     0     0     0     0]  \n",
    "[22382    42 46418    15   706]  \n",
    "[ 4505   505    15     3  3342]  \n",
    "[    0     0     0     0     0]\\]\n",
    ">- maintenant, que c'est fait, je peux vous le dire, il y a une fonction dans `keras.preprocessing` qui permet de le faire. La trouver et comparer les temps d'éxecution des 2 fonctions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Échantillons d'entraînement, de validation et de test**\n",
    "\n",
    "Découper les données en train, validation et test sets de la manière qui vous plaira. Tant que c'est juste et cohérent bien sûr.  \n",
    "Il faut qu'il y ait 20000 observations dans le train, 2500 dans le validation et 2500 dans le test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. Petite paranthèse sur le _word embedding_**\n",
    "\n",
    "Le [*word embedding*](https://fr.wikipedia.org/wiki/Word_embedding) consiste à transformer des mots sous forme de vecteurs de nombres. Bon ça on peut le faire relativement facilement, vous venez d'ailleurs de le faire avec la représentation du lexique des reviews en nombres entiers. Pour passer à un vecteur il suffirait juste de faire un one-hot-encoding.\n",
    "\n",
    "Il y a plusieurs problèmes à cette solution (même si on l'utilise parfois dans du NLP simple) :\n",
    "- la dimension de l'espace engendré\n",
    "- l'absence totale de notion de similarité (avec la représentation one-hot, 2 mots qui n'ont rien à voir sont aussi différents que 2 mots tout à fait synonyme)\n",
    "- le fait que les vecteurs contiennent quasiment que des 0 (sparse matrix)\n",
    "\n",
    "Donc le *word embedding* s'attaque à ce problème avec pour objectifs de :\n",
    "1. représenter les mots sous forme de vecteurs de nombres réels (et non entiers, on passe donc à un espace continu et plus discret)\n",
    "2. conserver la notion de similarité c'est-à-dire que 2 vecteurs qui sont proches doivent représenter des mots \"sémantiquement proches\"\n",
    "3. de les représenter dans un espace de plus petite dimension (en fonction de votre vocabulaire, soit le nombre de mots dans votre problème, ça peut aller très vite, généralement plusieurs milliers ou 10aines de milliers)\n",
    "\n",
    "Le principe est de s'intéresser au contexte des mots, c'est-à-dire, quels mots sont associés ensemble en s'appuyant sur la co-occurrence.\n",
    "Différents modèles de word embedding existent : [word2vec](https://fr.wikipedia.org/wiki/Word2vec), \n",
    "\n",
    "En pratique avec `keras`, que se passe-t-il lorsqu'on ajout une couche [*embedding*](https://keras.io/api/layers/core_layers/embedding/) ? Un exemple juste en dessous.\n",
    "\n",
    "Un peu de visionnage pour y voir plus clair si ça vous intéresse :\n",
    "- https://www.youtube.com/watch?v=Eku_pbZ3-Mw\n",
    "- https://www.youtube.com/watch?v=oUpuABKoElw\n",
    "- https://www.youtube.com/watch?v=5PL0TmQhItY\n",
    "\n",
    "Et un peu de lecture :\n",
    "- https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/\n",
    "- https://towardsdatascience.com/why-do-we-use-embeddings-in-nlp-2f20e1b632d2\n",
    "- https://towardsdatascience.com/word-embeddings-for-nlp-5b72991e01d4\n",
    "- https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf\n",
    "- https://medium.com/rasa-blog/supervised-word-vectors-from-scratch-in-rasa-nlu-6daf794efcd8\n",
    "- https://web.stanford.edu/~jurafsky/slp3/6.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on prend quelques phrases d'exemples pour illustrer le word embedding\n",
    "t1 = \"i hope to see you again\"\n",
    "t2 = \"you wish to see me soon\"\n",
    "t3 = \"i wish we will meet again\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6. Création, entraînement et évaluation du modèle**\n",
    "\n",
    "Créer un premier modèle, que vous serez tout à fait libre et même cordialement conviés à améliorer par la suite, avec :\n",
    "- une couche Embedding\n",
    "- une couche LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
