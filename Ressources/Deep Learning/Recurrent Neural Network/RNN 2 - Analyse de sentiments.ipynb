{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Recurrent Neural Network - Analyse de sentiments**\n",
    "\n",
    "Le but de cet exercice est la classification binaire (positive ou négative) de reviews en utilisant un RNN sur un dataset de commentaires IMDB que vous trouverez dans le dossier data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import des librairies\n",
    "import string\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Import des données et preprocessing**\n",
    "\n",
    "1. Importer les 2 fichiers \".txt\" de data/imdb\n",
    "2. Vérifier ce qu'il y a dedans si c'est pas déjà fait...\n",
    "3. Un peu de preprocessing de texte, allez on va chercher dans sa petite mémoire et sinon dans son gros ordinateur :\n",
    ">- convertir en minuscules\n",
    ">- retirer la ponctuation\n",
    ">- créer une liste des reviews et déterminer combien il y en a. Pareil pour les labels. Inch'Allah y en aura autant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import des txt\n",
    "with open('data/imdb/reviews.txt', 'r') as f:\n",
    "    reviews_str = f.read()\n",
    "with open('data/imdb/labels.txt', 'r') as f:\n",
    "    labels_str = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(str,\n",
       " 33678267,\n",
       " 'bromwell high is a cartoon comedy . it ran at the same time as some other programs about school life  such as  teachers  . my   years in the teaching ')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reviews\n",
    "type(reviews_str), len(reviews_str), reviews_str[:150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(str,\n",
       " 225000,\n",
       " 'positive\\nnegative\\npositive\\nnegative\\npositive\\nnegative\\npositive\\nnegative\\npositive\\nnegative\\npositive\\nnegative\\npositive\\nnegative\\npositive\\nnegative\\npositi')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# labels\n",
    "type(labels_str), len(labels_str), labels_str[:150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# netoyage des string\n",
    "reviews_str = reviews_str.lower()\n",
    "reviews_str = reviews_str.translate(str.maketrans('', '', string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 25000)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split et nb de lables\n",
    "rvws = reviews_str.splitlines()\n",
    "labs = labels_str.splitlines()\n",
    "len(rvws), len(labs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Tokenisation ou encodage**\n",
    "\n",
    "Le but est de remplacer les mots des reviews par des entiers. Si vous voulez faire à votre sauce, vous y êtes encouragés !\n",
    "\n",
    "Sinon, les quelques étapes décrites ci-dessous vous permettront de le faire :\n",
    ">- compter l'ensemble des mots (vous pouvez utiliser `Counter` de la librairie `collections` qui est une des plus rapide dans ce domaine)\n",
    ">- les trier par ordre décroissant d'occurrences\n",
    ">- créer un dictionnaire `word_mapping` où les clés sont les mots et les valeurs l'entier associé (votre dico doit être `{'the': 1, 'and': 2, 'a': 3, 'of': 4,..., 'muppified': 74070, 'whelk': 74071, 'hued': 74072}}`). C'est volontaire que ça commence à 1 car on utilisera le \"0\" comme caractère spécial (il servira pour \"remplir\" les reviews les plus courtes afin qu'elles aient toutes la même taille...)\n",
    ">- encoder les mots (c'est-à-dire les remplacer par l'entier qui les représente)\n",
    ">- encoder les labels (ça c'est fastoche, on se débrouille)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comptage des mots et tri par occurrence\n",
    "words = reviews_str.split()\n",
    "countwords = Counter(words)\n",
    "sorted_words = sorted(countwords, key=countwords.get, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionnaire de correspondance mot=entier\n",
    "word_mapping = {w:i+1 for i,w in enumerate(sorted_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[21025, 308, 6, 3, 1050, 207, 8, 2138, 32, 1, 171, 57, 15, 49, 81, 5785, 44, 382, 110, 140, 15, 5194, 60, 154, 9, 1, 4975, 5852, 475, 71, 5, 260, 12, 21025, 308, 13, 1978, 6, 74, 2395, 5, 613, 73, 6, 5194, 1, 24103, 5, 1983, 10166, 1, 5786, 1499, 36, 51, 66, 204, 145, 67, 1199, 5194, 19869, 1, 37442, 4, 1, 221, 883, 31, 2988, 71, 4, 1, 5787, 10, 686, 2, 67, 1499, 54, 10, 216, 1, 383, 9, 62, 3, 1406, 3686, 783, 5, 3483, 180, 1, 382, 10, 1212, 13583, 32, 308, 3, 349, 341, 2913, 10, 143, 127, 5, 7690, 30, 4, 129, 5194, 1406, 2326, 5, 21025, 308, 10, 528, 12, 109, 1448, 4, 60, 543, 102, 12, 21025, 308, 6, 227, 4146, 48, 3, 2211, 12, 8, 215, 23], [63, 4, 3, 125, 36, 47, 7472, 1395, 16, 3, 4181, 505, 45, 17, 3, 622, 134, 12, 6, 3, 1279, 457, 4, 1721, 207, 3, 10624, 7373, 300, 6, 667, 83, 35, 2116, 1086, 2989, 34, 1, 898, 46417, 4, 8, 13, 5096, 464, 8, 2656, 1721, 1, 221, 57, 17, 58, 794, 1297, 832, 228, 8, 43, 98, 123, 1469, 59, 147, 38, 1, 963, 142, 29, 667, 123, 1, 13584, 410, 61, 94, 1774, 306, 755, 5, 3, 819, 10396, 22, 3, 1724, 635, 8, 13, 128, 73, 21, 233, 102, 17, 49, 50, 617, 34, 682, 85, 28785, 28786, 682, 374, 3341, 11398, 2, 16371, 7946, 51, 29, 108, 3324]]\n"
     ]
    }
   ],
   "source": [
    "# encodage des mots dans les reviews\n",
    "rvws_num = [list(map(lambda w : word_mapping[w], rvw.split())) for rvw in rvws]\n",
    "print(rvws_num[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encodage des labels\n",
    "labs_num = [1*(lab == 'positive') for lab in labs]\n",
    "labs_num[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Longueur des séquences**\n",
    "\n",
    "Il s'agit ici d'analyser la longueur des reviews pour déterminer éventuellement des outliers et choisir ce qu'on en fait. On va aussi \"uniformiser\" la longueur des séquence.\n",
    "\n",
    "1. Avec la méthode de votre choix (graphique, stats desc...), étudier la longueur des reviews\n",
    "2. Déterminer s'il y a des outliers et ce que vous souhaitez en faire (si vous les décidez de les supprimer, attention de bien supprimer aussi les labels correspondants...)\n",
    "3. Ça a été évoqué un peu plus haut, on veut que nos reviews aient toutes la même taille pour faciliter l'entraînement du réseau. Par conséquent on va ajouter des 0 aux reviews les plus courtes et tronquer les reviews les plus longues (*padding/truncating*, si vous vous souvenez bien on a vu il y a peu le *zero-padding* dans un certain cas...bon ben c'est pareil).  \n",
    ">- définir une fonction `trunc_pad(review_list, length)` :\n",
    ">>- qui prend en paramètres la liste des reviews (chaque review étant encodée en une liste d'entiers) et une longueur donnée\n",
    ">>- et qui retourne un array 2D avec (en ligne) les reviews trop longues tronquées et des 0 à gauche pour les reviews trop courtes. Avant de vous lancer et pour vous assurer d'avoir compris, quelles dimensions doit avoir votre array en sortie ?\n",
    ">>- tester votre fonction avec une longueur fixée à 250 et afficher les 5 premières valeurs des 5 premières reviews. Vous devez obtenir ça:  \n",
    "\\[[    0     0     0     0     0]  \n",
    "[    0     0     0     0     0]  \n",
    "[22382    42 46418    15   706]  \n",
    "[ 4505   505    15     3  3342]  \n",
    "[    0     0     0     0     0]\\]\n",
    ">- maintenant, que c'est fait, je peux vous le dire, il y a une fonction dans `keras.preprocessing` qui permet de le faire. La trouver et comparer les temps d'éxecution des 2 fonctions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    25000.00000\n",
       "mean       240.80784\n",
       "std        179.01773\n",
       "min         10.00000\n",
       "25%        130.00000\n",
       "50%        179.00000\n",
       "75%        293.00000\n",
       "max       2514.00000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# la longueur des reviews\n",
    "rvws_len = pd.Series((map(len, rvws_num)))\n",
    "rvws_len.hist(bins=20);\n",
    "rvws_len.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# longueur des séquences\n",
    "seq_len = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fonction padding/truncating\n",
    "def trunc_pad(review_list=rvws_num, length=300):\n",
    "    out = np.zeros((len(review_list), length), dtype=int)\n",
    "    for i, rvw in enumerate(review_list):\n",
    "        out[i,-len(rvw):] = rvw[:length]\n",
    "        \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0, ...,     8,   215,    23],\n",
       "       [    0,     0,     0, ...,    29,   108,  3324],\n",
       "       [22382,    42, 46418, ...,    13,   391,    22],\n",
       "       [ 4505,   505,    15, ...,   108,    37,   227],\n",
       "       [    0,     0,     0, ...,     6,   179,   395]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "trunc_pad(length=seq_len)[:5, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 288 ms, sys: 3.8 ms, total: 291 ms\n",
      "Wall time: 289 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X0 = pad_sequences(rvws_num, maxlen=seq_len, truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 269 ms, sys: 7.64 ms, total: 276 ms\n",
      "Wall time: 274 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X = trunc_pad(review_list=rvws_num, length=seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Échantillons d'entraînement, de validation et de test**\n",
    "\n",
    "Découper les données en train, validation et test sets de la manière qui vous plaira. Tant que c'est juste et cohérent bien sûr.  \n",
    "Il faut qu'il y ait 20000 observations dans le train, 2500 dans le validation et 2500 dans le test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25000, 250), (25000,))"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array(labs_num)\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29.1 ms, sys: 4.6 ms, total: 33.7 ms\n",
      "Wall time: 31.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# méthode sklearn\n",
    "X_tmp, X_test, y_tmp, y_test = train_test_split(X, y, test_size=2500)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_tmp, y_tmp, test_size=2500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.4 ms, sys: 4.29 ms, total: 18.7 ms\n",
      "Wall time: 16.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# on peut le faire \"à la main\" aussi mais ne pas oublier de mélanger les données\n",
    "idx_shuffled = np.arange(25000)\n",
    "np.random.shuffle(idx_shuffled)\n",
    "X_shuffled = X[idx_shuffled]\n",
    "y_shuffled = y[idx_shuffled]\n",
    "\n",
    "nb_tr, nb_val = 20000, 2500\n",
    "X_train, X_val, X_test = X_shuffled[:nb_tr], X_shuffled[nb_tr:nb_tr+nb_val], X_shuffled[nb_tr+nb_val:]\n",
    "y_train, y_val, y_test = y_shuffled[:nb_tr], y_shuffled[nb_tr:nb_tr+nb_val], y_shuffled[nb_tr+nb_val:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 250) (2500, 250) (2500, 250)\n",
      "(20000,) (2500,) (2500,)\n"
     ]
    }
   ],
   "source": [
    "# on check les shapes\n",
    "print(X_train.shape, X_val.shape, X_test.shape)\n",
    "print(y_train.shape, y_val.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. Petite paranthèse sur le _word embedding_**\n",
    "\n",
    "Le [*word embedding*](https://fr.wikipedia.org/wiki/Word_embedding) consiste à transformer des mots sous forme de vecteurs de nombres. Bon ça on peut le faire relativement facilement, vous venez d'ailleurs de le faire avec la représentation du lexique des reviews en nombres entiers. Pour passer à un vecteur il suffirait juste de faire un one-hot-encoding.\n",
    "\n",
    "Il y a plusieurs problèmes à cette solution (même si on l'utilise parfois dans du NLP simple) :\n",
    "- la dimension de l'espace engendré\n",
    "- l'absence totale de notion de similarité (avec la représentation one-hot, 2 mots qui n'ont rien à voir sont aussi différents que 2 mots tout à fait synonyme)\n",
    "- le fait que les vecteurs contiennent quasiment que des 0 (sparse matrix)\n",
    "- le fait que les modèles sont ensuite difficilement généralisable car en cas de nouveau mot, le modèle ne sait pas du tout les traiter puisqu'il ne peut pas du tout les comparer aux mots vus dans l'entraînement\n",
    "\n",
    "Donc le *word embedding* s'attaque à ce problème avec pour objectifs de :\n",
    "1. représenter les mots sous forme de vecteurs de nombres réels (et non entiers, on passe donc à un espace continu et plus discret)\n",
    "2. conserver la notion de similarité c'est-à-dire que 2 vecteurs qui sont proches doivent représenter des mots \"sémantiquement proches\"\n",
    "3. de les représenter dans un espace de plus petite dimension (en fonction de votre vocabulaire, soit le nombre de mots dans votre problème, ça peut aller très vite, généralement plusieurs milliers ou 10aines de milliers)\n",
    "\n",
    "Le principe est de s'intéresser au contexte des mots, c'est-à-dire, quels mots sont associés ensemble en s'appuyant sur la co-occurrence.\n",
    "Différents modèles de word embedding existent : [word2vec](https://fr.wikipedia.org/wiki/Word2vec), Glove, fasttext...\n",
    "\n",
    "En pratique avec `keras`, que se passe-t-il lorsqu'on ajout une couche [*embedding*](https://keras.io/api/layers/core_layers/embedding/) ? Un exemple juste en dessous.\n",
    "\n",
    "Un peu de visionnage pour y voir plus clair si ça vous intéresse :\n",
    "- https://www.youtube.com/watch?v=Eku_pbZ3-Mw\n",
    "- https://www.youtube.com/watch?v=oUpuABKoElw\n",
    "- https://www.youtube.com/watch?v=5PL0TmQhItY\n",
    "\n",
    "Et un peu de lecture :\n",
    "- https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/\n",
    "- https://towardsdatascience.com/why-do-we-use-embeddings-in-nlp-2f20e1b632d2\n",
    "- https://towardsdatascience.com/word-embeddings-for-nlp-5b72991e01d4\n",
    "- https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf\n",
    "- https://medium.com/rasa-blog/supervised-word-vectors-from-scratch-in-rasa-nlu-6daf794efcd8\n",
    "- https://web.stanford.edu/~jurafsky/slp3/6.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on prend quelques phrases d'exemples pour illustrer le word embedding\n",
    "t1 = \"i hope to see you again\"\n",
    "t2 = \"you wish to see me soon\"\n",
    "t3 = \"i wish we will meet again\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'i': 1,\n",
       "  'to': 2,\n",
       "  'see': 3,\n",
       "  'you': 4,\n",
       "  'again': 5,\n",
       "  'wish': 6,\n",
       "  'hope': 7,\n",
       "  'me': 8,\n",
       "  'soon': 9,\n",
       "  'we': 10,\n",
       "  'will': 11,\n",
       "  'meet': 12},\n",
       " 12)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# méthode mise en place au dessus\n",
    "cnts = Counter(' '.join([t1,t2,t3]).split())\n",
    "dico = {w:i+1 for i,w in enumerate(sorted(cnts, key=cnts.get, reverse=True))}\n",
    "dico, len(dico)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 7, 2, 3, 4, 5], [4, 6, 2, 3, 8, 9], [1, 6, 10, 11, 12, 5]]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# en représentant les vecteurs avec cette méthode on obtient\n",
    "t_num = [[dico[w] for w in t.split()] for t in [t1, t2, t3]]\n",
    "t_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 6, 2)              26        \n",
      "=================================================================\n",
      "Total params: 26\n",
      "Trainable params: 26\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:6 out of the last 12 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f7637d2daf0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[[[ 1.21878982e-02 -2.65855677e-02]\n",
      "  [ 2.03719474e-02  8.20383430e-05]\n",
      "  [ 3.71456854e-02 -9.21390206e-03]\n",
      "  [ 2.59860642e-02  1.91943385e-02]\n",
      "  [ 1.03757605e-02 -2.71544456e-02]\n",
      "  [ 3.49335335e-02  2.88656987e-02]]] [[[ 0.01037576 -0.02715445]\n",
      "  [ 0.03473543 -0.01919251]\n",
      "  [ 0.03714569 -0.0092139 ]\n",
      "  [ 0.02598606  0.01919434]\n",
      "  [-0.00295806  0.04078201]\n",
      "  [-0.03978165 -0.02091659]]] [[[ 0.0121879  -0.02658557]\n",
      "  [ 0.03473543 -0.01919251]\n",
      "  [-0.02161729  0.04306128]\n",
      "  [-0.01042342  0.01372781]\n",
      "  [ 0.00705602  0.00491847]\n",
      "  [ 0.03493353  0.0288657 ]]]\n"
     ]
    }
   ],
   "source": [
    "# on crée un modèle avec un couche embedding\n",
    "mod_emb = Sequential()\n",
    "mod_emb.add(Embedding(input_dim=13, output_dim=2, input_length=6))\n",
    "mod_emb.compile()\n",
    "print(mod_emb.summary())\n",
    "\n",
    "out1 = mod_emb.predict([t_num[0]])\n",
    "out2 = mod_emb.predict([t_num[1]])\n",
    "out3 = mod_emb.predict([t_num[2]])\n",
    "\n",
    "print(out1, out2, out3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6. Création, entraînement et évaluation du modèle**\n",
    "\n",
    "Créer un premier modèle, que vous serez tout à fait libre et même cordialement conviés à améliorer par la suite, avec :\n",
    "- une couche Embedding\n",
    "- une couche LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_11 (Embedding)     (None, 250, 64)           4740672   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               66000     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 4,806,773\n",
      "Trainable params: 4,806,773\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=Sequential()\n",
    "\n",
    "model.add(Embedding(input_dim=len(word_mapping)+1, output_dim=64, input_length=seq_len))\n",
    "model.add(LSTM(units=100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "625/625 [==============================] - 78s 125ms/step - loss: 0.4852 - accuracy: 0.7612 - val_loss: 0.4593 - val_accuracy: 0.8352\n",
      "Epoch 2/3\n",
      "625/625 [==============================] - 75s 120ms/step - loss: 0.2712 - accuracy: 0.8961 - val_loss: 0.3398 - val_accuracy: 0.8632\n",
      "Epoch 3/3\n",
      "625/625 [==============================] - 76s 121ms/step - loss: 0.1441 - accuracy: 0.9497 - val_loss: 0.3703 - val_accuracy: 0.8660\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f763d6b3d00>"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), batch_size=32, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8763999938964844\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f'Accuracy: {scores[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
